{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Downloading stopwords and wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Loading dataset\n",
    "data = pd.read_csv('news_dataset.csv')\n",
    "\n",
    "# Dropping null values\n",
    "data = data.dropna()\n",
    "\n",
    "# Cleaning the text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenization, removing stop words, and lemmatization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Collection\n",
    "\n",
    "The dataset typically consists of labeled news articles with a binary label indicating whether the news is real or fake. The dataset might have columns such as title, text, label, where label indicates if the news is real or fake.\n",
    "Step 2: Data Preprocessing\n",
    "\n",
    "Preprocessing is crucial to clean and prepare the data for analysis. The following steps are involved:\n",
    "\n",
    "    Removing Null Values: Drop rows with null values as they can affect the analysis.\n",
    "    Text Cleaning: Remove unnecessary characters, stop words, punctuation, and perform stemming or lemmatization. This step helps in standardizing the text.\n",
    "    Tokenization: Convert the text into a list of words (tokens).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(data['text']).toarray()\n",
    "y = data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Training the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis Steps\n",
    "\n",
    "    Data Understanding: Understand the structure and content of the dataset. Check for class imbalance.\n",
    "    Data Cleaning: Remove irrelevant characters, stop words, and perform text normalization.\n",
    "    Feature Engineering: Convert text data into numerical features using TF-IDF vectorization.\n",
    "    Model Selection: Choose a suitable model for classification. Logistic Regression is a good start due to its simplicity and effectiveness for text classification.\n",
    "    Model Training and Testing: Train the model on the training set and evaluate it on the test set.\n",
    "    Performance Evaluation: Use metrics like accuracy, confusion matrix, and classification report to evaluate the model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
